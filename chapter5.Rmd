---
output: html_document
editor_options: 
  chunk_output_type: inline
---
# Chapter 5 Dimensionality reduction techniques

*Summary of the week (completed work and learning)*

In this Chapter We cover two parts:\

* Part 1 data wrangling: we continue the R script create_human.R.\

* Part 2 data analysis: we analyze the data of Human Development Index (HDI) which originates from the United Nations Development Program. We utilize Principal Component Analysis (PCA) and Multiple Correspondence Analysis (MCA) techniques.\

We use the following R packages: 

```{r, results="hide", message=FALSE, warning=FALSE, error=FALSE}
library(ggplot2) # to produce plots
library(GGally)
library(ggpubr)  # this is to combine plots in one frame
library(dplyr)
library(tidyr)   # to plot multiple densities and histograms
#library(caret)   # to perform cross validations
```

## 5.1 Graphical overview of the data

The dataset is about Human Development Index (HDI) composed from a number of indicators measuring several various dimensions as described here: http://hdr.undp.org/en/content/human-development-index-hdi and here: http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf for more technical details.

The dataset covers 155 countries and consists of 8 variables, i.e. \
(1) ratio of mean years of education between female and male ("Edu2.FM") \
(2) ratio of labor force participation rate between female and female ("Labo.FM") \
(3) life expectancy at birth ("Life.Exp") \
(4) expected years of education ("Edu.Exp") \
(5) Gross National Income per capita ("GNI") \
(6) maternal mortality ratio ("Mat.Mor") \
(7) adolescent birth rate ("Ado.Birth"), and \ 
(8) percentage of female representation in Parliament ("Parli.F").

We present summaries of the variables in the data below. From the structure of the dataset, all variables are numerical variables of which 7 variables are double precision floating point numbers and 1 variable is integer. Based on the summary, we can see that we have a diverse range of series, except between "Edu2.FM" and "Labo.FM" as both are ratios.

```{r}
# load the data
human <- read.csv('./data/human.csv', row.names=1)
str(human) # explore the structure
summary(human)
```
\
We now display a graphical overview of each variable together with their correlations between them. From the distribution plots, we have only "Edu.Exp" and "Parli.F" which are close to density of a normal distribution. 

In terms of the relationships between them, many variables are significantly correlated to each other. The highest positive correlation is between "Edu.Exp" and "Life.Exp" (0.789) and the highest negative correlation is between "Mat.Mor" and "Life.Exp" (-0.857). All the correlations between variables can be seen from the outputs of "ggpairs" and "ggcorr" R packages below.


```{r}
ggpairs(human)
```

```{r}
ggcorr(human, palette = "RdBu", label = TRUE, label_round=2, label_size = 3) 
```

## 5.2 Perform principal component analysis (PCA)

We continue by performing principal component analysis (PCA) on the originally constructed series (not standardized) of human data. If we look at the summary, the Cumulative Proportion shows that until the second Principal Component (PC2), it is already 98.74%, hence the 8 variables may be reduced to 2 variables only. 

```{r}
pca_human <- prcomp(human)
summary(pca_human)

# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

```{r}
# rounded percentages of variance captured by each PC
s <- summary(pca_human)
pca_pr <- round(100*s$importance[2,], digits = 1) #rounded percentages of variance captured by each PC
pca_pr   #print out the percentages of variance
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)") #create object pc_lab to be used as axis labels
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```
\
In the figures above, we show the variability captured by the principal components and we draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables. We can see that the first two principal components account for 98.74% proportion. 

## 5.3 Standardize the variables and perform PCA

We now standardize the variables in the human data and repeat the principal component analysis. 

```{r}
human_std <- scale(human) #standardize the variables
pca_human_std <- prcomp(human_std)
summary(pca_human_std)

# draw a biplot of the principal component representation and the original variables
biplot(pca_human_std, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
```

```{r}
# rounded percentages of variance captured by each PC
s <- summary(pca_human_std)
pca_pr <- round(100*s$importance[2,], digits = 1) #rounded percentages of variance captured by each PC
pca_pr   #print out the percentages of variance
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)") #create object pc_lab to be used as axis labels
# draw a biplot
biplot(pca_human_std, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```
\
Comparing both analysis (with and without standardizing), we can see somewhat different results. Without standardizing, the first two principal components already account for 98.74%. However, now with standardizing, it needs 7 principal components to account for 98.79% of the whole variance. It means that with standardizing, the 8-variable model can now only be reduced to 7 variables. 
If we look again the densities of the variables in the beginning, variables "GNI" and "Mat.Mor" are the two variables that feature the most non-normal distributions and we can see the divergence of these two variables in the biplot of the one without standardizing. With standardizing, it is not the case since all variables are now normally distributed with zero mean and unit variance. Hence, the results from with and without standardizing are different.

## 5.4 Personal interpretations 

Here we focus to interpret the biplot of the PCA on the standardized data. The plot shows the first two principal component scores and the loading vectors in a single biplot display. First, the two components (PC1 and PC2) account for 66.28% of the total variability of the whole 8-variable model where PC1 explains 49.8% of variance and PC2 explains 16.4% of the variance. Second, it seems that PCA output is concentrated more on positive side. Then, the arrows are plotted at a scale related to the maximum ratio between the scaled eigenvectors of each of the two principal components and their respective scaled scores (the ratio).


## 5.5 The tea dataset from the package 'Factominer'

We now load the tea dataset from the package Factominer. We explore the data briefly by looking at the structure and the dimensions of the data and then visualize it. 

```{r, results="hide", message=FALSE, warning=FALSE, error=FALSE}
library(FactoMineR)
library(factoextra)
```

```{r, results="hide", message=FALSE, warning=FALSE, error=FALSE}
data("tea") #load the tea dataset
str(tea) #explore the structure
summary(tea)
dim(tea) #300 obs. of 36 variables
```
\
We can group the data into three groups:\

(1) Integer variables: "age" and "age_Q".\
(2) Variables with 2 categories (yes/not): "breakfast", "tea.time", "evening", "lunch", "dinner", "always", "home", "work", "tearoom", "friends", "resto", "pub", "sugar", "sex", "Sport", "escape.exoticism", "spirituality", "healthy", "diuretic", "friendliness", "iron.absorption", "feminine", "sophisticated", "slimming", "exciting", "relaxing", "effect.on.health".\  
(3) Variables with more than 2 categories: "Tea", "How", "how", "where", "price", "SPC", "frequency".\

We then Visualize the tea data here:

```{r}

group1 <- c("age", "age_Q")
group2 <- c("breakfast", "tea.time", "evening", "lunch", "dinner", "always", "home", "work", "tearoom", "friends", "resto", "pub", "sugar", "sex", "Sport", "escape.exoticism", "spirituality", "healthy", "diuretic", "friendliness", "iron.absorption", "feminine", "sophisticated", "slimming", "exciting", "relaxing", "effect.on.health")
group3 <- c("Tea", "How", "how", "where", "price", "SPC", "frequency")

#Visualize group 1
plot(tea$age,tea$age_Q)
legend("topleft", inset=.02, title="Group of ages",
   c("1: 15-24","2: 25-34","3: 35-44","4: 45-59","5: +60"), horiz=FALSE, cex=0.8)
```

```{r}
#Visualize group 2 (selected variables)

breakfast <- data.frame(tea$tea.time,tea$breakfast)
eat1 = ggplot(breakfast, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$breakfast), position = "dodge")
lunch <- data.frame(tea$tea.time,tea$lunch)
eat2 = ggplot(lunch, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$lunch), position = "dodge")
dinner <- data.frame(tea$tea.time,tea$dinner)
eat3 = ggplot(dinner, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$dinner), position = "dodge")

ggarrange(eat1, eat2, eat3,
          ncol = 1, nrow = 3)
```
```{r}
#Visualize group 2 (selected variables)
home <- data.frame(tea$tea.time,tea$home)
at1 = ggplot(home, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$home), position = "dodge")
work <- data.frame(tea$tea.time,tea$work)
at2 = ggplot(work, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$work), position = "dodge")

ggarrange(at1, at2,
          ncol = 1, nrow = 2)
```
```{r}
#Visualize group 2 (selected variables)

f1 <- data.frame(tea$tea.time,tea$escape.exoticism)
feel1 = ggplot(f1, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$escape.exoticism), position = "dodge")
f2 <- data.frame(tea$tea.time,tea$friendliness)
feel2 = ggplot(f2, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$friendliness), position = "dodge")
f3 <- data.frame(tea$tea.time,tea$exciting)
feel3 = ggplot(f3, aes(tea$tea.time, ..count..)) + geom_bar(aes(fill = tea$exciting), position = "dodge")

ggarrange(feel1, feel2, feel3,
          ncol = 1, nrow = 3)
```

```{r}
#Visualize group 3 (selected variables)
group3 <- c("Tea", "How", "how", "where", "price", "SPC", "frequency")

br = ggplot(tea, aes(x=reorder(Tea, Tea, function(x)-length(x)))) +
geom_bar(fill='blue') +  labs(x='Tea')
tt = ggplot(tea, aes(x=reorder(How, How, function(x)-length(x)))) +
geom_bar(fill='blue') +  labs(x='How')
ev = ggplot(tea, aes(x=reorder(how, how, function(x)-length(x)))) +
geom_bar(fill='blue') +  labs(x='how')
lu = ggplot(tea, aes(x=reorder(where, where, function(x)-length(x)))) +
geom_bar(fill='blue') +  labs(x='where')
din = ggplot(tea, aes(x=reorder(price, price, function(x)-length(x)))) +
geom_bar(fill='blue') +  labs(x='price')
fr = ggplot(tea, aes(x=reorder(frequency, frequency, function(x)-length(x)))) +
geom_bar(fill='blue') +  labs(x='frequency')

ggarrange(br, tt, ev, lu, din, fr,
          ncol = 2, nrow = 3)

```
\
We now perform Multiple Correspondence Analysis (MCA) on the tea data (to a certain columns of the data: only 6 columns of "Tea", "How", "how", "sugar", "where", "breakfast"). 

MCA is generally used to analyze a data set from survey. The goal is to identify: (1) A group of individuals with similar profile in their answers to the questions; and (2) The associations between variable categories. (Source: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/)

```{r}
# Multiple Correspondence Analysis on the tea data

teatime <- data.frame(tea$Tea,tea$How,tea$how,tea$sugar,tea$where,tea$breakfast)
summary(teatime)
str(teatime)

# multiple correspondence analysis
mca <- MCA(teatime, graph = FALSE)

# summary of the model
summary(mca)
```
\
The summary above displays eigenvalues including computed proportion of variance, the 10 first individuals, the 10 first categories, and the categorical variables. Let's now we visualize and interpret using the _factoextra_ R package that we have loaded previously.

First, we visualize the percentages of inertia explained by each MCA dimensions. We can interpret, for instance, to explain at least 50% of variances, we need at least 4 dimensions (15.3%+14.5%+11.8%+10.6%=52.2%).

```{r}
# visualize MCA
fviz_screeplot(mca, addlabels = TRUE, ylim = c(0, 20))

```
\
Next, we draw the biplot of individuals and variable categories. The biplot below shows a global pattern within the data. Rows (individuals) are represented by blue points and columns (variable categories) by red triangles. The distances between the points, either between column points or row points, measure the similarity between them. For example, individual 191 is very similar to individual 249 and individual 22 is very dissimilar to individual 31 or 168.

```{r}
# visualize MCA
fviz_mca_biplot(mca, 
               repel = TRUE, # Avoid text overlapping (slow if many point)
               ggtheme = theme_minimal())

```
\
We can also visualize the correlation between variables and MCA principal dimensions as follows. We can see that, for instance, tea.breakfast is more correlated with dimension 2, while te.where is correlated with both dimensions.

```{r}
# visualize MCA
fviz_mca_var(mca, choice = "mca.cor", 
            repel = TRUE, # Avoid text overlapping (slow)
            ggtheme = theme_minimal())

```
\
W can also highlight the most important (or contributing) variable categories on the plot below. We can see that 'unpacked' and 'tea shop' contribute to the positive pole of dimension 1, while negative to the dimension 2. Vice versa for 'milk' and 'breakfast'.

```{r}
fviz_mca_var(mca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, # avoid text overlapping (slow)
             ggtheme = theme_minimal()
             )
```
\
We can also visualize the individuals by groups using for example the levels of the variable 'breakfast'. While 'Not.breakfast' is more to dimension 1, 'breakfast' is more to dimension 2.

```{r}
# visualize MCA
fviz_mca_ind(mca, 
             label = "none", # hide individual labels
             habillage = "tea.breakfast", # color by groups 
             palette = c("#00AFBB", "#E7B800"),
             addEllipses = TRUE, ellipse.type = "confidence",
             ggtheme = theme_minimal()) 

```
\
Finally, we can visualize individuals using multiple categorical variables at the same time. We can see that 'tea shop' contributes positively to dimension 1, while negative to dimension 2. On the other hand, 'chain store' contributes negatively to both dimension 1 and dimension 2.

```{r}
# visualize MCA
fviz_ellipses(mca, c("tea.How", "tea.sugar", "tea.where"),
              geom = "point")

```

\
P.S. Visualizations in this MCA Section are mostly adopted from http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/.

_This is the end of the report_

```{r}
date()

```


